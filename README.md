# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The dataset contains row of 10000 people who were contacted via telephone or cellular. The data contains information about education, age, job, martial status and some other variables. It also contains the variable "y" with "no" and "yes" as values. This seems to be a variable describing some form of success either regarding a marketing campaing or something similar. We will try to predict the outcome of "y" based on the ohter variables.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The data is prepared by "train.py". A pandas dataframe is created, NULL navules are removed and some variables are formated correctly. Then the data is split into 1/3 testing data and 2/3 training data. We have two hyperparameters to optimize our model with: "C" and "max_iter" and use a logistic regression model as classification model.

**What are the benefits of the parameter sampler you chose?**

I picked a uniform sampler for "C" and a choice sampler for "max_iter". The benefit of this is to provide a defined range in which HypderDrive will then find optimal parameters for the model.

**What are the benefits of the early stopping policy you chose?**

The early stopping policy is usefull to be efficient with ressources. The bandit policy will abort once is seems unlikely that a run will lead to a better solution.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

For AutoML I only define that I want to train  a classification model. AutoML will then try different models and optimize the hyperparameters of these models to find an optimal model  based on the metric I pick.
With my configuration the solution was: "VotingEnsemble".

The following hyperparameters were used:
experiment_timeout_minutes=30   --> maximum time to look for an optimal model. BEst model after 30 minutes will be picked as best solution.
task='classification'           --> model type
primary_metric='accuracy'       --> metric used to evaluate the model performance
training_data=x                 --> dataframe used to train the model on
label_column_name='y'           --> column name wich contains the possible outcomes for the classification
n_cross_validations=3           --> nnumber of cross validations

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

With HyperDrive I limit the best possible solution by the model and the random search for parameter values. However, with my configuration of HyperDrive and the Logistic regression classifier, AutoML was only able to find a slightly better solution. The "accuracy" of the AutoML model was with a value of 0.913 mostly as good as the HyperDrive solution with an accuracy of 0.914 I would conclude that in this case the LogisticRegression model was a good choice.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

The configuration of HyperDrive and AutoML could be optimized by someone with more experience. This way, possible better solutions are not avoided from the beginning and other non-optimal solutions are avoided to be more efficient with ressources.
For example the "XGBoostCLassifier", "VotingEnsemble" and "StackEnsemble" seem to be very potent to solve this problem with the highest accuracy. It might be worth it to optimize these models and block alternatives completly, so we spend more time optimizing the most promesing models instead of also considering models that don't seem to have the same potential for this problem.
The hyperparameter tuning can also be either narrowed down to find the last bit of minimal optimisation after seeing the results from the first run. Or the range for the hyperparameters could be even extended if the first results suggest that the first definition might have excluded the best options.
